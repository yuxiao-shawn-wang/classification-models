---
title: "MRKT 9653 MS ML Homework 2"
author: "Yuxiao Wang yw3739"
date: "2/28/2020"
output:
  pdf_document: default
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = F)
setwd("~/Downloads/MSBA/2020 Spring/MS ML/HW/hw2")
library(glmnet)
library(naniar)
library(dplyr)
library(tree)
library(randomForest)
library(gbm)
library(caret)
```

## Question 1
According to the documentation of this dataset, the variable duration is unknown before a call, so it will not be useful if we want to predict the purchase intent of customers and should be discarded.

The variables month and day_of_week are indicators of dates of last contact and it hardly makes any sense to say that making a call on a Monday in May will improve the probability that a customer will purchase the product. Also, adding these variables might cause overfitting of the model.

The nr.employed is the fact regarding this bank, which has little to do with customers' intention. Even if we want to use it as an indicator of social and economic environment, the standard deviation of this variable is relatively samll compared to its actual value, which makes no big difference throughout the time peroid of this dataset.

Therefore, we should discard the above 4 variables mentioned.

```{r q1-1}
# read dataset
bank0 <- read.table("bank-additional-full.csv", 
                    sep=";", 
                    header = TRUE, 
                    stringsAsFactors = FALSE)
summary(bank0[,c("month", "day_of_week", "duration", "nr.employed")])
```

The rest of the dataset gives us information about the client, effect from previous campaigns and general economic environment.

A client is described with her age, job, marital status, education level, whether having credit in default and whether having a houseing loan or a personal loan. According to the summary, the sample clients are mostly middle-aged, married clients with university degrees and no loans nor default credit.

Effect of current and previous campaigns is described with contact method, number of contacts performed, days since last contact and outcome of last campaign. In general, contacts are mostly made with cell phones for 2-3 times in last campaign, which is performed almost 2.5 years ago. Only 3% of customers accepted the offers during last campaign.

The economic environment is described with employment variation rate, consumer price index, consumer confidence index and euribor 3 month rate. During this period, the employment variation rate, the consumer confidence index and the euribor 3 month rate varies a lot while the consumer price index does not change much.

The interested variable in prediction is y, which is the binary variable indicating whether a customer subscribed a term deposit or not.

```{r q1-2}
# take a subset that excludes month, day of week, duration and nr.employed
bank <- subset(bank0, 
               select = -c(month, day_of_week, duration, nr.employed))
summary(bank)
```


## Question 2
After removing rows with "unknown" observations, we have 30,488 records in the dataset.

For "job" attribute, we treat "student", "unemployed" and "retired" as "unemployment" while others as "employment".

For "marital" attribute, we treat anything other than "married" as "single".

For "education" attribute, we conbine "illiterate" and "basic.4y" since "illiterate" has only 11 value counts. Then we replace the original values with 1-6 according to the level of education.

The results are shown in the following.

```{r q2}
# replace unknown with NA
df <- bank %>% na_if("unknown") 
# drop NA
df <- na.omit(df)

# convert job into two levels: employed, unemployed
unemployed <-  c("student", "unemployed", "retired")
df <- df %>% mutate(job = replace(job, job %in% unemployed, "unemployed"))
df <- df %>% mutate(job = replace(job, job != "unemployed", "employed"))
count(df,job)

# convert marital into two levels: single, married
df <- df %>% mutate(marital = replace(marital, marital != "married", "single"))
count(df,marital)

# convert education into numeric orderd dummy variable
# conbine illiterate with basic.4y
df$education <- as.character(df$education)
df <- df %>% mutate(education = replace(education, 
                                        education=="illiterate", 
                                        as.character(0)))
education_level <- c("basic.4y", "basic.6y", "basic.9y", 
                     "high.school", "professional.course", "university.degree")
for (edu in education_level) {
  n <- which(education_level==edu)
  df <- df %>% mutate(education = replace(education, 
                                          education==edu, 
                                          as.character(n)))
}
df$education <- as.numeric(df$education)
count(df,education)
```


## Question 3
After converting all string variables into factors, the result dataset is summarized in the following.

```{r q3}
# split rows into train/test evenly
set.seed(1)
train <- sample( 1: nrow(df),nrow(df)/2) 
test <- -train

# convert variables into factors
df$job <- as.factor(df$job)
df$marital <- as.factor(df$marital)
df$default <- as.factor(df$default)
df$housing <- as.factor(df$housing)
df$loan <- as.factor(df$loan)
df$contact <- as.factor(df$contact)
df$poutcome <- as.factor(df$poutcome)
df$y <- as.factor(df$y)
summary(df)

train.df <- df[train,]
test.df <- df[test,]
```

## Question 4
The classification tree using gini results in a deeper tree with more child nodes, while the tree using deviance is simpler. However, as the "mindev" is set to a extremely smaller number, the tree using deviance is similar to that using gini.

Gini Tree:

```{r q4-1}
# train a classification tree with gini criteria
gini.tree <- tree(y ~., data = train.df, split = "gini", 
                  control = tree.control(length(train), 
                                         mincut = 15, minsize = 30))
plot(gini.tree)
text(gini.tree, pretty = 0, cex = .5)
```

Deviance Tree:

```{r q4-2}
# train a classification tree with deviance criteria
dev.tree <- tree(y ~., data = train.df, split = "deviance", 
                 control = tree.control(length(train), mindev = 0.01))
plot(dev.tree)
text(dev.tree, pretty = 0, cex = .5)
```

Deviance Tree (mindev = 0.001):

```{r q4-3}
# train a classification tree with deviance and mindev = 0.001
dev.tree2 <- tree(y ~., data = train.df, split = "deviance", 
                  control = tree.control(length(train), mindev = 0.001))
plot(dev.tree2)
text(dev.tree2, pretty = 0, cex = .5)
```

## Question 5
The variable importance of the random forest model is summarized in the table below. The variable importance graph is also shown in the below.
```{r q5}
# train a random forest model
rf.tree <- randomForest(y ~., data = train.df, importance = TRUE)

# display variable importance
importance(rf.tree)
varImpPlot(rf.tree, main="Random Forest")
```

## Question 6
The variable importance of the boostded tree model is summarized in the table below. The variable importance graph is also shown in the below.

```{r q6,message=F}
# train a boosted tree model
# convert Y into numeric 0 and 1
train.Y <- as.numeric(train.df$y) -1

# train boost tree with 3-fold cross validation
objControl <- trainControl(method='cv', number=3, 
                           returnResamp='none', 
                           classProbs = TRUE)

boost.tree <- train(train.df[,-17], train.df[,17], 
                  method='gbm', 
                  trControl=objControl,  
                  metric = "Accuracy",
                  verbose = FALSE)

# display variable importance
summary(boost.tree, las = 2, cBars = 16)
```

## Question 7
As described below, boosted tree seems to have the best performance in terms of accuracy. 

All the models have comparable accuracy results. Surprisingly, the random forest model does not outperform the simple regression tree using deviance criteria. 

However, all these models did not achieve a significant improvement in accuracy compared with the benchmark accuracy (classify everything as "no" will result in a 87.48% of accuracy).

Therefore, we can conclude that for this dataset, a boosted tree has the best performance of accuracy, but it might not be very useful given the unbalanced sizes of two categories in "y".

Also, more complex models do not always result in better results.
```{r q7}
# prepare test set
Y.test <- test.df$y

# calculate prediction accuracy on test set

# simple classification tree with gini criteria
gini.pred <- predict(gini.tree, test.df, type="class")
r1 <- table(gini.pred, Y.test)
cat("Accuracy of Gini Tree:", (r1[1,1]+r1[2,2])/length(Y.test), "\n")

# simple classification tree with deviance criteria
dev.pred <- predict(dev.tree, test.df, type="class")
r2 <- table(dev.pred, Y.test)
cat("Accuracy of Deviance Tree:", (r2[1,1]+r2[2,2])/length(Y.test), "\n")

# random forest
rf.pred <- predict(rf.tree, test.df, type="class")
r3 <- table(rf.pred, Y.test)
cat("Accuracy of Random Forest:", (r3[1,1]+r3[2,2])/length(Y.test), "\n")

# boosted tree
boost.pred <- predict(boost.tree, test.df, n.trees = best.iter, type="raw")
r4 <- table(boost.pred, Y.test)
cat("Accuracy of Boosted Tree:", (r4[1,1]+r4[2,2])/length(Y.test), "\n")

# benchmark: if predcit every record as "no"
neg <- length(Y.test[Y.test=="no"])
cat("Accuracy Benchmark:", neg/length(Y.test), "\n")
```


## Appendix. R Code
```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```
